\thispagestyle{empty}

\section{Softwarelösungen}

\subparagraph{Linux-VServer}
Linux-VServer ist eine der ältesten Implementierungen von Linux-Container-basierten Systemen. Anstatt Namespaces zu verwenden, verwendet Linux-VServer selbst eingeführte Funktionen im Linux-Kernel, um die Isolation zu gewährleisten, wie z.B. Prozessisolation, Netzwerkisolation und CPU-Isolation. Linux-VServer verwendet den traditionellen Aufruf des Chroot-Systems, um das Filesystem innerhalb der Container einzusperren. Auf diese Weise schränkt es den Umfang des Filesystems für die Prozesse ein. Die Prozessisolierung erfolgt durch einen globalen PID-Raum, der alle Prozesse außerhalb des Bereichs eines Containers verbirgt und unerwünschte Verbindungen zwischen Prozessen verschiedener Container verhindert. Das wichtigste Merkmal dieses Ansatzes ist die Skalierbarkeit für eine große Anzahl von Containern. Der Nachteil ist jedoch die Unfähigkeit des Systems, die üblichen Virtualisierungstechniken wie Live-Migration, Checkpoint und Wiederherstellung zu implementieren, da es nicht möglich ist, Prozesse mit derselben PID neu zu initiieren. Linux-VServer virtualisiert keine Netzwerk-Subsysteme. Vielmehr werden alle Netzwerk-Subsysteme (wie Routingtabellen und IP-Tabellen) von allen Containern gemeinsam genutzt. Dieser Ansatz setzt einen Identifier-Tag um zu vermeiden, dass ein Container Netzwerkverkehr zu anderen Containern empfangen kann. Entsprechende Filter sind im Netzwerkstack hinterlegt, um sicherzustellen, dass nur der richtige Container die Daten empfangen kann. Die Container sind nicht in der Lage, ihre eigenen Routingtabellen- und IP-Tabellen zu ändern, was vom Host-Administrator durchgeführt werden muss. Um die CPU-Isolation zu gewährleisten, verwendet Linux-V Server den Standard-Linux-Scheduler, der durch das \emph{Token Bucket Filter} (TBF) Schema überlagert wird. Jedem Container ist ein Token Bucket zugeordnet, der dazu dient, Token mit einer bestimmten Geschwindigkeit zu sammeln. Auf diese Weise ist jeder Prozess bei der Erstellung eines Tokens mit einem Container verbunden. Diese Token werden bei der Ausführung auf der CPU im Container Bucket bis zu einer bestimmten Mindest- oder Maximalanzahl gesammelt. Dieses Token Bucket Schema kann verwendet werden, um eine faire Aufteilung der CPU zu gewährleisten. Ressourcenbegrenzungen wie Speicherverbrauch und Anzahl der Prozesse werden mit Systemaufrufen (rlimit-Tool) des Linux-Kernels durchgeführt. Die neuesten Versionen von Linux-VServer bieten jedoch Unterstützung für Cgroup, mit denen auch die CPU-Auslastung und der Speicherverbrauch von Containern eingeschränkt werden können. Die Linux-VServer Container werden vom \emph{util-vserver} \cite{Optionen2018Userspace-WerkzeugeLinux-VServer} Package verwaltet \cite{Overview2018PaperLinux-VServer} \cite{Xavier2015AClouds}.

\subparagraph{OpenVZ}
OpenVZ bietet eine ähnliche Funktionalität wie Linux-VServer. Es basiert jedoch auf Kernel-Namespaces, wodurch es sicherstellt, dass jeder Container seine eigene isolierte Teilmenge einer Ressource erhält. Das System verwendet PID-Namespaces, um die Prozessisolierung zwischen Containern zu gewährleisten. Darüber hinaus ermöglichen PID-Namespaces übliche Virtualisierungstechniken wie Live-Migration, Checkpoint und Wiederherstellungs Methoden. In OpenVZ hat dank IPC-Namespaces jeder Container seinen eigenen Semaphoren und Nachrichtenspeicher. Außerdem werden Network-Namespaces genutzt. OpenVZ verwendet vier Ressourcenmanagement Komponenten, \emph{User Beancounter} (UBS), faires CPU-Scheduling, \emph{Disk Quota} und I/O-Scheduling. UBS bietet die Möglichkeit der Limitierung der Ressourcen, die auf jedem Container kontrolliert werden. Der OpenVZ CPU-Scheduler funktioniert auf zwei Arten, um eine faire Verteilung zu generieren. Zum Einen wird entschieden, welcher Container als nächstes auf dem Prozessor ausgeführt werden darf, zum Anderen wird anhand einer Prioritätenliste die Reihenfolge der Prozesse des Containers erstellt. Ein weiteres Konzept ist die \emph{VCPU Affinity}, was die maximale Anzahl an CPUs definiert, die ein Container verwenden darf. \emph{Disk Quota} ist eine Funktion, mit der eine Begrenzung des Speicherplatzes auf einem Speichermedium für einzelne Benutzer oder eine Gruppe von Benutzern festgelegt werden kann. Schließlich wird ein ähnlicher Ansatz des CPU-Scheduling für I/O-Scheduling verwendet. Das Verfahren ist wieder in zwei Teile aufgeteilt. Der erste Teil funktioniert genau wie beim CPU-Scheduling, der zweite Teil wird anhand des \emph{Completely Fair Queuing} (CFQ) Algorithmus festgelegt. Jeder Container besitzt eine I/O-Priorität, und der I/O-Scheduler verteilt die verwendbare Bandbreite anhand der Priorisierung. Auf diese Weise kann nicht ein einzelner Container den Kompletten Kanal beanspruchen. OpenVZ Container werden durch \emph{vzctl} \cite{ParallelsIPHoldingsGMbH2018Vzctl} verwaltet \cite{IndexOpenvz.org} \cite{Xavier2015AClouds}.

\subparagraph{Linux Container}
Ähnlich wie OpenVZ verwendet LXC Kernel-Namespaces, um die Ressourcenisolierung zwischen allen Containern zu gewährleisten. Während des Container Starts werden standardmäßig PIDs, IPCs und Mount Points virtualisiert und über den PID-Namespace, den IPC-Namespace bzw. den Mount-Namespace isoliert. Um mit der Außenwelt zu kommunizieren und die Netzwerkisolierung zu ermöglichen, verwendet LXC die Netzwerk-Namespaces. Im Gegensatz zu Linux-VServer und OpenVZ ist die Ressourcenverwaltung nur über Cgroups erlaubt. Die Prozesskontrolle wird ebenfalls über Cgroups durchgeführt. I/O-Operationen sind wie in OpenVZ ebenfalls durch den CFQ-Scheduler gesteuert \cite{IndexLinuxcontainers.Org} \cite{Xavier2015AClouds}.

\subparagraph {Xen}
Xen ist eine Virtualisierungs Lösung, die ursprünglich an der University of Cambridge entwickelt wurde. Xen ist die einzige Bare-Metall-Lösung, die als Open Source als Grundlage für eine Reihe verschiedener kommerzieller und Open Source Anwendungen dient. Xen besteht aus mehreren Komponenten, die zusammenwirken, um eine Virtualisierungsumgebung bereitzustellen. Die Hauptbestandteile sind Xen-Hypervisor, Domain0 Gast (Dom0) und DomainU Gast (DomU), die entweder Para-virtualisiert, vollständig-virtualisiert oder hardwareunterstützt-virtualisiert sein können. Der Xen-Hypervisor ist eine Softwareschicht, die direkt auf der Hardware unter allen Betriebssystemen läuft. Er ist für das CPU-Scheduling und die Speicherpartitionierung der verschiedenen VMs, die auf der Hardwarevorrichtung laufen zuständig. Wenn Xen startet, übernimmt der Xen-Hypervisor die Kontrolle über das System und lädt das erste Gastbetriebssystem auf Dom0. Dom0 ist ein modifizierter Linux-Kernel, der eine einzigartige Virtuelle Maschine darstellt und auf dem Xen-Hypervisor mit speziellen Zugriffsrechten auf physische I/O-Ressourcen, sowie die Interaktion mit anderen Virtuellen Maschinen läuft. Die auf der DomU ausgeführten modifizierten Betriebssysteme besitzen keinen direkten Zugriff auf physische Daten. Seit Xen Version 3.0 ist \emph{CREDIT} der Standard Xen-Scheduler um die CPU fair auf die Virtuellen Maschinen aufzuteilen. Im Credit-Scheduler wird jeder VM eine Gewichtung zugewiesen, anhand derer die CPU-Ressourcen auf die Systeme verteilt werden \cite{Fayyad-Kazan2013BenchmarkingHypervisors}. 

\subparagraph {VMware Workstation}
VMware Workstation wurde 1998 entwickelt um die x86 Architektur zu virtualisieren ohne Hardware oder Software zu ändern. Infolgedessen unterscheidet sich VMware Workstation von den klassischen Überwachungen Vitueller Maschinen. Um die Virtualisierung in bestehende Systeme einzubenden, kombiniert VMware Workstation eine gehostete Architektur mit einem Virtual Machine Monitor (VMM). Die gehostete Architektur ermöglicht eine einfache Benutzerführung und bietet eine breite Hardwarekompatibilität. Die Architektur ermöglicht es, bei minimaler Beeinträchtigung den gemeinsamen Vorsitz auf Systemebene zwischen einem Host-Betriebssystem und einem VMM zu übernehmen. Anstatt die I/O-Diversität der x86-Plattform den Virtuellen Maschinen auszusetzen, setz VMware Workstation auf die SoftwareEmulation von kanonisch ausgewählten I/O-Geräten und ermöglicht damit auch die hardwareunabhängige Kapselung von Virtuellen Maschinen. Der VMWare VMM kompensiert den Mangel an architektonischer Unterstützung für die Virtualisierung durch die Kombination einer Direct Execution Engine mit einem Binärübersetzerr auf Systemebene, um die x86-Architektur effizient zu virtualisieren und die meisten gängigen Betriebssysteme zu unterstützen. Der VMM verwendet die Segmentierung als Schutzmechanismus, sodass sein binär übersetzter Code mit nahezu Hardware-Geschwindigkeiten ausgeführt werden kann. Der VMM bietet auch adaptive binäre Übersetzung, um den Overhead der Virtualisierung von Speicher stark zu reduzieren \cite{Bugnion2012BringingWorkstation}.


\subparagraph{KVM}
\emph{Kernel Virtual Machine} (KVM) ist eine Funktion von Linux, die es ermöglicht, als Typ-1-Hypervisor zu fungieren und ein unmodifiziertes Gastbetriebssystem in einem Linux Prozess zu erstellen. KVM verwendet Hardware-Virtualisierung in aktuellen Prozessoren, um die Komplexität und den Overhead zu reduzieren. Bei Intel VT-x oder AMD-v erübrigt sich die Notwendigkeit einer komplexen Ring-Rechteverwaltung, welches von früheren Hypervisoren wie Xen und VMware verwendet wurde. KVM verwendet sowohl über \emph{QEMU} \cite{QEMUEmulator}" emulierte, als auch via \emph{virtio} \cite{View2018VirtioVirtio} paravirtualisierte I/O-Geräte. Die Kombination aus Hardwarebeschleunigung und paravirtuellem I/O ist entwickelt worden, um den Virtualisierungs-Overhead auf sehr niedrige Werte zu reduzieren. KVM unterstützt die Live-Migration und ermöglicht dadurch die Wartung von physischen Servern oder sogar ganzen Rechenzentren, ohne dass ein Gastbetriebssystem unterbrochen wird. Da eine VM eine statische Anzahl von virtuellen CPUs (vCPUs) hat und eine feste Menge an Arbeitsspeicher, ist der Ressourcenverbrauch natürlich begrenzt. Eine vCPU stellt einen echten CPU-Wert der Zyklen dar und jede Memory-Page des virtuellen Arbeitsspeichers stellt genau eine Memory-Page des realen Rechners dar. KVM kann die Größe von VMs während des Betriebs durch \emph{Hotplugging} und \emph{Ballooning} durch die dafür benötigte Betriebssystemunterstützung verändern. Da jede VM ein Prozess ist, gelten alle normalen Linux-Ressourcenverwaltungsfunktionen in den Virtuellen Maschinen. Das vereinfacht zwar die Implementierung und Verwaltung des Hypervisor, erschwert aber die Ressourcenverwaltung innerhalb des Gastbetriebssystems. Betriebssysteme gehen im Allgemeinen davon aus, dass CPUs immer laufen und der Speicher eine relativ feste Zugriffszeit hat. Unter KVM können vCPUs jedoch ohne Benachrichtigung ausgeplant werden, was zu Performance-Anomalien führt, die schwer zu debuggen sind. Viele Cloud-Anbieter beseitigen diese Probleme, indem sie keine übermäßigen Ressourcen binden, jede vCPU an eine physische CPU koppeln und den gesamten virtuellen Arbeitsspeicher auf den Realen zuschneiden. Dadurch entfällt im Wesentlichen die Planung im Hypervisor. VMs bieten ein gewisses Maß an Isolierung und Sicherheit durch ihre schmale Schnittstelle. Der einzige Weg, über die eine VM mit der Außenwelt kommunizieren kann, ist über eine begrenzte Anzahl von \emph{Hypercalls} oder emulierten Geräten, die beide vom Hypervisor gesteuert werden. Doch auch das ist keine perfekte Lösung. Es wurden Hypervisor-Privilegien-Eskalationsschwachstellen entdeckt, die es einem Gastbetriebssystem ermöglichen konnten aus seiner VM \emph{Sandbox} auszubrechen \cite{Felter2014IBMContainers}.

\subparagraph{Hyper-V}
Microsoft Hyper-V ist ein in Windows integrierter Hypervisor, der die Möglichkeit für Para-Virtualisierung und hardwareunterstütze-Virtualisierung bereitstellt. Hyper-V gibt es zudem als eigenständige Version namens Hyper-V Server und als installierbares Produkt \emph{Windows Server} Es gibt keine Unterschiede in der Funktionsweise zwischen den Microsoft Hyper-V. Der Hypervisor ist derselbe, unabhängig von der installierten Variante. MS Hyper-V implementiert die Isolierung von virtuellen Maschinen in Form von Partitionen. Hyper-V unterstütz Para-Virtualisierung und hardwareunterstützte-Virtualisierung. Genau wie bei KVM kann der Performance Overhead reduziert werden. Die Architektur des Hyper-V funktioniert anhand von \emph{micro-kernelized} Hypervisoren. Bei dieser Architektur wird ein Parent/Child System verwendet, wobei die Parent-Partition das Management von Hardware-geräten organisiert. Das original erstellte Windows Betriebssystem stellt die Parent Partition dar, alle weiteren neu erstellten Virtuellen Maschinen bilden die sogenannten Child-Partitionen. Jeder Zugriff auf virtuelle Geräte wird vom VMBus dirigiert. VMBus ist ein logischer Chanel welcher die inter-Partitions-Kommunikation verhindern und nur mit berechtigten Partitionen interagiert. Auch der durch Hyper-V zur Verfügung gestellt Hypervisor konnte von einem Benutzer durchbrochen werden, der Benztzer ist aus der \emph{Sandbox} des Gastbetriebssystems entkommen und hat privilegierten Zugriff auf Ressourcen erhalten \cite{Fayyad-Kazan2013BenchmarkingHypervisors}.
