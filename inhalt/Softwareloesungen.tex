\thispagestyle{empty}

\section{Softwarelösungen}

\subsection{Container-based-Software}

\subparagraph{Linux-V Server}
Linux-VServer ist eine der ältesten Implementierungen von Linux-Container-basierten Systemen. Anstatt Namespaces zu verwenden, verwendet Linux-VServer selbst eingeführte Funktionen im Linux-Kernel um die Isolation zu gewähleisten, wie z.B. Prozessisolation, Netzwerkisolation und CPU-Isolation. Linux-VServer verwendet den tratidionellen Aufruf des Chroot-Systems, um das File-System innerhalb der Container einzusperren. Auf diese Weise schränkt es den Umfang des File-Systems für die Prozesse ein. Die Prozessisolierung erfolgt durch einen globalen PID-Raum, der alle Prozesse außerhalb des Bereichs eines Containers verbirgt und unerwünschte Verbindungen zwischen Prozessen verschiedener Container verhindert. Das wichtigste Merkmal dieses Ansatzes ist die Skalierbarkeit für eine große Anzahl von Containern. Der Nachteil ist jedoch die Unfähigkeit des Systems die üblichen Virtualisierungstechniken wie Live-Migration, Checkpoint und Wiederherstellung zu implementieren, da es nicht möglich ist, Prozesse mit derselben PID neu zu initiieren. Linux-VServer virtualisiert keine Netzwerk-Subsysteme. Vielmehr werden alle Netzwerk-Subsysteme (wie Routingtabellen und IP-Tabellen) von allen Containern gemeinsam genutzt. Dieser Ansatz setzt einen Identifier-Tag um zu vermeiden, dass ein Container Netzwerkverkehr zu anderen Containern empfangen kann. Entsprechende Filter sind im Netzwerk stack hinterlegt, um sicherzustellen, dass nur der richtige Container die Daten empfangen kann. Die Container sind nicht in der Lage ihre eigenen Routingtabellen-und IP-Tabellen zu ändern, was vom Host-Administrator durchgeführt werden muss. Um die CPU-Isolation zu gewährleisten, verwendet Linux-VServer den Standard-Linux-Scheduler, der durch das Token Bucket Filter (TBF) Schema überlagert wird. Jedem Container ist ein Token Bucket zugeordnet, der dazu dient, Token mit einer bestimmten Geschwindigkeit zu sammeln. Auf diese Weise ist jeder Prozess, bei der Erstellung eines Token mit einem Container verbunden. Diese Token werden bei der Ausführung auf der CPU im Container-Bucket bis zu einer bestimmten Mindest- oder Maximalanzahl gesammelt. Dieses Token-Bucket-Schema kann verwendet werden, um eine faire Aufteilung der CPU zu gewährleisten. Ressourcenbegrenzungen wie Speicherverbrauch und Anzahl der Prozesse werden mit Systemaufrufen (rlimit-Tool) des Linux-Kernels durchgeführt. Die neuesten Versionen von Linux-VServer bieten jedoch Unterstützung für Cgroups, mit denen auch die CPU-Auslastung und der Speicherverbrauch von Containern eingeschränkt werden können. Die Linux-VServer-Container werden vom util-vserver\cite{Optionen2018Userspace-WerkzeugeLinux-VServer} package verwaltet \cite{Overview2018PaperLinux-VServer} \cite{Xavier2015AClouds}.

\subparagraph{Open VZ}
penVZ bietet eine ähnliche Funktionalität wie Linux-VServer. Es basiert jedoch auf Kernel-Namespaces, wodurch es sicherstellt, das jeder Container seine eigene isolierte Teilmenge einer Ressource erhällt. Das System verwendet PID-Namespaces, um die Prozessisolierung zwischen Containern zu gewährleisten. Darüber hinaus ermöglichen PID-Namespaces übliche Virtualisierungstechniken wie Live-Migration, Checkpoint und Widerherstellungs Methoden. In OpenVZ hat dank IPC-Namespaces jeder Container sein eigenes Speicher, Semaphoren und Nachrichten Segment. Außerdem werden Network-Namespaces genutzt. OpenVZ verwendet vier Ressourcenmanagement Komponenten, "User Beancounter" (UBS), faires CPU-Scheduling, "Disk Quatas" und I/O-scheduling. UBS bietet die Möglichkeit der Limitierung der Ressourcen, die auf jedem Container kontrolliert werden. Der OpenVZ CPU scheduler funktioniert auf zwei arten um eine faire verteilung zu generieren. Zum einen wird entschieden welcher Container als nächstes auf dem Processor ausgeführt werden darf, zum anderen wird anhand einer Prioritätenliste die Reihenfolge der Prozesse des Containers erstellt. Es gibt eine weiteres Konzept ist die "VCPU Affinity", was die maximale Anzahl an CPUs definiert, die ein Container verwenden darf. "Disk Quota" ist eine Funktion, mit der eine Begrenzung des Speicherplatzes auf einem Speichermedium für einzelne Benutzer oder eine Gruppe von Benutzern festgelegt werden kann. Schließlich wird ein ähnlicher Ansatz des CPU-scheduling für I/O-scheduling verwendet. Das Verfahren ist wieder in zwei Teile aufgeteilt, der erste Teil funktioniert genau wie beim CPU-scheduling, der zweite teil wird anhand des "Completely Fair Queuing" (CFQ) Algorithmus festgelegt. Jeder Container besitzt eine I/O-Priorität, und der I/O-Scheduler verteilt die verwendbare Bandbreite anhand der Priorisierung. Auf diese weise kann nicht ein Einzelner Container die Kompletten Kanal beanspruchen. OpenVZ Container werden durch "vzctl"\cite{ParallelsIPHoldingsGMbH2018Vzctl} verwaltet\cite{IndexOpenvz.org}\cite{Xavier2015AClouds}.

\subparagraph{Linux Container}
Ähnlich wie Open VZ verwendet LXC Kernel-Namespaces, um die Ressourcenisolierung zwischen allen Containern zu gewährleisten. Während des Container Starts, werden stanardmäßig PIDs, IPCs und Mount Points virtualisiert und über den PID-Namespace, den IPC-Namespace bzw. den Mount-Namespace isoliert. Um mit der Außenwelt zu kommunizieren und die Netzwerkisolierung zu ermöglichen, werden verwendet LXC die Nezwerk-Namespaces. Im gegensatz zu Linux-VServer und OpenVZ ist die Ressourcenverwaltung nur über Cgroups erlaubt. Die Prozesskontrolle wird ebenfalls über Cgroups durchgeführt. I/O Operationen sind wie in OpenVZ ebenfalls durch den CFQ-scheduler gesteuert \cite{IndexLinuxcontainers.Org} \cite{Xavier2015AClouds}.



\subsection{Hypervisor-based-Software}

\paragraph{Typ1VMM}
\subparagraph {VMware ESX}


\paragraph{Typ2VMM}
\subparagraph {VMware Workstation}
\subparagraph{Virtual Box}

\subparagraph {Xen}
